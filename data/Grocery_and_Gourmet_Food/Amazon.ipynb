{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 0: 常量、依赖与随机种子 ====\n",
    "import os, json, gzip, urllib.request, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# 数据集名\n",
    "DATASET = \"Grocery_and_Gourmet_Food\"\n",
    "\n",
    "# --- 智能路径逻辑 Start ---\n",
    "current_path = Path.cwd()\n",
    "\n",
    "# 判断：如果当前文件夹的名字已经是数据集的名字，说明脚本就在数据目录内部\n",
    "if current_path.name == DATASET:\n",
    "    print(\"检测到脚本运行在数据目录下，使用当前目录作为 DATA_DIR。\")\n",
    "    DATA_DIR = current_path\n",
    "    ROOT = current_path.parents[1] # 尝试推断项目根目录（往上两级），仅作记录用\n",
    "else:\n",
    "    print(\"检测到脚本运行在项目根目录（或其他位置），将创建 data 子目录。\")\n",
    "    ROOT = current_path\n",
    "    DATA_DIR = ROOT / \"data\" / DATASET\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# --- 智能路径逻辑 End ---\n",
    "\n",
    "# 统一随机种子\n",
    "RANDOM_SEED = 2025\n",
    "NEG_ITEMS   = 100 \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"ROOT (项目根目录): {ROOT}\")\n",
    "print(f\"DATA_DIR (数据目录): {DATA_DIR}\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db607b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 1: 下载原始数据（若已存在则跳过） ====\n",
    "DATA_FILE = f\"reviews_{DATASET}_5.json.gz\"\n",
    "META_FILE = f\"meta_{DATASET}.json.gz\"\n",
    "\n",
    "sources = [\n",
    "    (DATA_FILE, f\"https://snap.stanford.edu/data/amazon/productGraph/categoryFiles/{DATA_FILE}\"),\n",
    "    (META_FILE, f\"https://snap.stanford.edu/data/amazon/productGraph/categoryFiles/{META_FILE}\"),\n",
    "]\n",
    "\n",
    "for fname, url in sources:\n",
    "    out = DATA_DIR / fname\n",
    "    if out.exists():\n",
    "        print(\"[exist]\", out.name)\n",
    "    else:\n",
    "        print(\"[download]\", url, \"->\", out)\n",
    "        urllib.request.urlretrieve(url, out)\n",
    "\n",
    "print(\"Done. Files under:\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 2: 解析 gz → DataFrame ====\n",
    "def parse_gz_json(path: Path):\n",
    "    with gzip.open(path, 'rb') as g:\n",
    "        for line in g:\n",
    "            yield eval(line)  # Amazon 官方示例使用的简便写法\n",
    "\n",
    "def load_df(path: Path) -> pd.DataFrame:\n",
    "    data = {}\n",
    "    for i, obj in enumerate(parse_gz_json(path)):\n",
    "        data[i] = obj\n",
    "    return pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "\n",
    "review_fp = DATA_DIR / f\"reviews_{DATASET}_5.json.gz\"\n",
    "meta_fp   = DATA_DIR / f\"meta_{DATASET}.json.gz\"\n",
    "\n",
    "reviews = load_df(review_fp)\n",
    "meta    = load_df(meta_fp)\n",
    "\n",
    "print(\"reviews:\", reviews.shape, \"columns:\", list(reviews.columns)[:8])\n",
    "print(\"meta   :\", meta.shape,    \"columns:\", list(meta.columns)[:8])\n",
    "\n",
    "# 只保留交互中出现过的物品\n",
    "meta_use = meta[meta['asin'].isin(reviews['asin'])].reset_index(drop=True)\n",
    "all_asins = set(meta_use['asin'])\n",
    "\n",
    "def related_filter(related_dict):\n",
    "    out = {}\n",
    "    if isinstance(related_dict, dict):\n",
    "        for k, lst in related_dict.items():\n",
    "            out[k] = list(all_asins & set(lst))\n",
    "    return out\n",
    "\n",
    "if 'related' in meta_use.columns:\n",
    "    meta_use['related'] = meta_use['related'].apply(related_filter)\n",
    "else:\n",
    "    meta_use['related'] = [{} for _ in range(len(meta_use))]\n",
    "\n",
    "print(\"#Users:\", reviews['reviewerID'].nunique(),\n",
    "      \"#Items:\", reviews['asin'].nunique(),\n",
    "      \"#Interactions:\", len(reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e910c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 3: 构造交互表（逗号 CSV：user_id,item_id,time）+ item_meta ====\n",
    "# 1) 重命名 → 去重 → 排序\n",
    "inter = reviews.rename(columns={'reviewerID':'user_id','asin':'item_id','unixReviewTime':'time'})\n",
    "inter = inter[['user_id','item_id','time']].drop_duplicates()\n",
    "inter = inter.sort_values(by=['time','user_id'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "# 2) 重新编号（从 1 开始）\n",
    "uid_list = sorted(inter['user_id'].unique())\n",
    "iid_list = sorted(inter['item_id'].unique())\n",
    "user2id = dict(zip(uid_list, range(1, len(uid_list)+1)))\n",
    "item2id = dict(zip(iid_list, range(1, len(iid_list)+1)))\n",
    "\n",
    "inter['user_id'] = inter['user_id'].map(user2id).astype(int)\n",
    "inter['item_id'] = inter['item_id'].map(item2id).astype(int)\n",
    "inter['time']    = inter['time'].astype(int)\n",
    "\n",
    "# 3) leave-one-out：每个用户最后两条做 test/dev，其余做 train（再把每个用户最早一条加入 train，保持稳定）\n",
    "clicked_item_set = {u: set(g['item_id'].tolist()) for u, g in inter.groupby('user_id')}\n",
    "\n",
    "def make_dev_test(df: pd.DataFrame):\n",
    "    res = []\n",
    "    n_items = df['item_id'].nunique()\n",
    "    work = df.copy()\n",
    "    for _ in range(2):  # test/dev 各取最后一条\n",
    "        last = work.groupby('user_id').tail(1).copy()\n",
    "        work = work.drop(last.index)\n",
    "        # （可选）保留一个 neg_items 列（后面会重采，这里不强依赖）\n",
    "        res.append(last)\n",
    "    return (res[0], res[1], work)  # test, dev, remain\n",
    "\n",
    "# 每个用户的第一条保底放回 train\n",
    "head1 = inter.groupby('user_id').head(1)\n",
    "rest  = inter.drop(head1.index)\n",
    "test_df, dev_df, remain_df = make_dev_test(rest)\n",
    "train_df = pd.concat([head1, remain_df]).sort_index()\n",
    "\n",
    "# 4) 保存“标准化的逗号 CSV”（框架中间产物；最终训练会用下面生成的子目录）\n",
    "train_csv = DATA_DIR / \"train.csv\"\n",
    "dev_csv   = DATA_DIR / \"dev.csv\"\n",
    "test_csv  = DATA_DIR / \"test.csv\"\n",
    "\n",
    "train_df[['user_id','item_id','time']].to_csv(train_csv, index=False)  # 逗号\n",
    "dev_df[['user_id','item_id','time']].to_csv(dev_csv, index=False)\n",
    "test_df[['user_id','item_id','time']].to_csv(test_csv, index=False)\n",
    "\n",
    "print(\"[base csv] saved ->\", train_csv, dev_csv, test_csv)\n",
    "\n",
    "# 5) 物品侧信息（逗号 CSV）\n",
    "#   二级品类\n",
    "l2 = []\n",
    "cats = meta_use.get('categories', [])\n",
    "for row in cats:\n",
    "    if isinstance(row, list) and len(row)>0 and len(row[0])>2:\n",
    "        l2.append(row[0][2])\n",
    "    else:\n",
    "        l2.append(np.nan)\n",
    "meta_use['l2_category'] = l2\n",
    "l2_vals = sorted(meta_use['l2_category'].dropna().unique())\n",
    "l2_map  = dict(zip(l2_vals, range(1, len(l2_vals)+1)))\n",
    "meta_use['l2_category'] = meta_use['l2_category'].apply(lambda x: l2_map[x] if x==x else 0)\n",
    "\n",
    "#   映射 related 到新 item_id 空间\n",
    "item_meta_rows = []\n",
    "for _, r in meta_use.iterrows():\n",
    "    asin = r['asin']\n",
    "    if asin not in item2id:\n",
    "        continue\n",
    "    info = r['related'] if isinstance(r['related'], dict) else {}\n",
    "    item_meta_rows.append({\n",
    "        'item_id'     : item2id[asin],\n",
    "        'i_category'  : r['l2_category'],\n",
    "        'r_complement': [item2id[x] for x in info.get('also_bought', []) if x in item2id],\n",
    "        'r_substitute': [item2id[x] for x in info.get('also_viewed', []) if x in item2id],\n",
    "    })\n",
    "item_meta = pd.DataFrame(item_meta_rows, columns=['item_id','i_category','r_complement','r_substitute'])\n",
    "item_meta_csv = DATA_DIR / \"item_meta.csv\"\n",
    "item_meta.to_csv(item_meta_csv, index=False)\n",
    "print(\"[item meta] saved ->\", item_meta_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f34cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 4: 生成 GGFTOPK 与 GGFCTR（TSV） ====\n",
    "TOPK_DIR = DATA_DIR / \"GGFTOPK\"\n",
    "CTR_DIR  = DATA_DIR / \"GGFCTR\"\n",
    "TOPK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CTR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 读取标准化 base csv（逗号）\n",
    "base = {\n",
    "    \"train\": pd.read_csv(DATA_DIR / \"train.csv\"),\n",
    "    \"dev\"  : pd.read_csv(DATA_DIR / \"dev.csv\"),\n",
    "    \"test\" : pd.read_csv(DATA_DIR / \"test.csv\"),\n",
    "}\n",
    "for k in base:\n",
    "    base[k] = base[k][[\"user_id\",\"item_id\",\"time\"]].astype({\"user_id\":int,\"item_id\":int,\"time\":int})\n",
    "\n",
    "# 全局候选 & 用户历史（用于负采样）\n",
    "all_df    = pd.concat(base.values(), ignore_index=True)\n",
    "all_items = np.asarray(sorted(all_df[\"item_id\"].unique()), dtype=int)\n",
    "user_hist = defaultdict(set)\n",
    "for u, i in zip(all_df[\"user_id\"].values, all_df[\"item_id\"].values):\n",
    "    user_hist[int(u)].add(int(i))\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "def candidate_pool(u: int) -> np.ndarray:\n",
    "    seen = user_hist[u]\n",
    "    if not seen:\n",
    "        return all_items\n",
    "    cand = np.setdiff1d(all_items, np.fromiter(seen, dtype=int), assume_unique=False)\n",
    "    return cand if cand.size>0 else all_items\n",
    "\n",
    "# ====== A) GGFTOPK：为 train/dev/test 每条样本生成 NEG_ITEMS 个未交互负例（列名 neg_items） ======\n",
    "for split in [\"train\",\"dev\",\"test\"]:\n",
    "    df = base[split].copy()\n",
    "    neg_lists = []\n",
    "    for u in df[\"user_id\"].values:\n",
    "        cand = candidate_pool(int(u))\n",
    "        neg_lists.append(list(rng.choice(cand, size=NEG_ITEMS, replace=True).astype(int)))\n",
    "    df[\"neg_items\"] = neg_lists\n",
    "    # **关键**：ReChorus 这套脚本在 Amazon 上常用 TSV\n",
    "    df.to_csv(TOPK_DIR / f\"{split}.csv\", index=False, sep=\"\\t\")\n",
    "    print(f\"[GGFTOPK] {split}.csv ->\", df.shape, list(df.columns))\n",
    "\n",
    "# ====== B) GGFCTR：1:1 负采样（含 label），保留 time；同样用 TSV ======\n",
    "def build_ctr(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pos = df[[\"user_id\",\"item_id\",\"time\"]].copy().astype(int)\n",
    "    pos[\"label\"] = 1\n",
    "    neg_rows = []\n",
    "    for u, t in zip(pos[\"user_id\"].values, pos[\"time\"].values):\n",
    "        cand = candidate_pool(int(u))\n",
    "        j = int(rng.choice(cand))\n",
    "        neg_rows.append((int(u), j, int(t), 0))\n",
    "    neg = pd.DataFrame(neg_rows, columns=[\"user_id\",\"item_id\",\"time\",\"label\"])\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "for split in [\"train\",\"dev\",\"test\"]:\n",
    "    out = build_ctr(base[split])\n",
    "    out.to_csv(CTR_DIR / f\"{split}.csv\", index=False, sep=\"\\t\")\n",
    "    print(f\"[GGFCTR ] {split}.csv ->\", out.shape, list(out.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15196957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 5: 快速自检 ====\n",
    "def peek(path: Path, n=2):\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "    print(path.name, df.shape, list(df.columns))\n",
    "    display(df.head(n))\n",
    "\n",
    "print(\"== GGFTOPK ==\")\n",
    "for sp in [\"train\",\"dev\",\"test\"]:\n",
    "    peek((DATA_DIR / \"GGFTOPK\" / f\"{sp}.csv\"))\n",
    "\n",
    "print(\"== GGFCTR ==\")\n",
    "for sp in [\"train\",\"dev\",\"test\"]:\n",
    "    peek((DATA_DIR / \"GGFCTR\" / f\"{sp}.csv\"))\n",
    "\n",
    "print(\"\\n✅ 准备就绪：接下来直接用 --dataset \\\"Grocery_and_Gourmet_Food/GGFTOPK\\\" 或 \\\"Grocery_and_Gourmet_Food/GGFCTR\\\" 训练即可。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chorus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
